{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/3\n",
    "class UnNormalize():\n",
    "    \"\"\"정규화 되었던 이미지를 복원하는 Transforms Function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(saved_model, model:str, num_classes, device, **kwargs):\n",
    "    model_cls = getattr(import_module(\"model.model\"), model)\n",
    "    model = model_cls(\n",
    "        num_classes=num_classes,\n",
    "        # 아래는 model에 추가적인 파라미터가 필요할 때 사용하시면 됩니다.\n",
    "        # **kwargs\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(saved_model, 'best.pth')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def inference_model(loader, model_dir, model:str, num_classes, break_idx=300, **kwargs) -> list:\n",
    "    \"\"\"Confusion Matrix 출력 함수.\n",
    "\n",
    "    Args:\n",
    "        loader (dataloader)): dataloader\n",
    "        model_dir (str): 저장된 모델 경로.\n",
    "        num_classes (int): 클래스 갯수\n",
    "        model (str): 모델 클래스명. 단, 문자열로 줘야 함.\n",
    "        break_idx (int, optional): 모두 다 inference하는 것은 너무 오래 걸려서 끊어주기 위한 장치를 마련. Defaults to 300.\n",
    "        **kwargs : 모델에 들어가게 되는 추가적인 파라미터들.\n",
    "    Returns:\n",
    "        list: 튜플 (wrong_imgs, wrong_answers, wrong_labels)을 담아낸 리스트. 이미지에 종속된 라벨을 한꺼번에 묶음.\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    model = load_model(model_dir, model, num_classes, device, **kwargs).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Calculating inference results for {model_dir}..\")\n",
    "    \n",
    "    answers = []\n",
    "    preds = []\n",
    "    \n",
    "    # 오답 처리된 이미지들에 대해 (이미지, 정답, 오답)을 출력하기 위한 과정들.\n",
    "    wrong_imgs, wrong_answers, wrong_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in enumerate(tqdm(loader)):\n",
    "            # <계산 과정>\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            pred = pred.argmax(dim=-1).cpu()\n",
    "            # </계산 과정>\n",
    "            \n",
    "            answers.extend(labels)\n",
    "            preds.extend(pred.numpy()) \n",
    "            \n",
    "            # 오답 판별 과정.\n",
    "            wrong_ans = (pred != labels)\n",
    "            \n",
    "            # 오답 판별 결과 누적.\n",
    "            wrong_imgs.extend(images[wrong_ans].cpu())\n",
    "            wrong_answers.extend(labels[wrong_ans].cpu())\n",
    "            wrong_labels.extend(pred[wrong_ans].cpu())\n",
    "            \n",
    "            # if idx >= break_idx:\n",
    "            #     break\n",
    "\n",
    "    return list(answers, preds, zip(wrong_imgs, wrong_answers, wrong_labels))\n",
    "    \n",
    "\n",
    "def display_cfmatrix(model_label, num_classes, answers, preds):\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    map_label = {\n",
    "        'gender':{0:'male', 1:'female'}, \n",
    "        'age':{0:'<30', 1:'>=30 and <60', 2:'>60'}, \n",
    "        'mask':{0:'mask', 1:'incorrect', 2:'normal'}\n",
    "    }\n",
    "    \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "    # sklearn.metrics에서의 confusion_matrix 메서드 사용.\n",
    "    cf_matrix = confusion_matrix(answers, preds, labels=list(range(num_classes)))\n",
    "    sns.heatmap(cf_matrix, annot=True)\n",
    "    \n",
    "    # confusion_matrix의 첫 파라미터는 y축으로 두 번째 파라미터는 x축으로 감.\n",
    "    ax.xaxis.set_ticklabels(map_label[model_label].values())\n",
    "    ax.yaxis.set_ticklabels(map_label[model_label].values())\n",
    "    plt.xlabel('pred', fontsize=14)\n",
    "    plt.ylabel('answer', fontsize=14)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "import random\n",
    "\n",
    "def show(pic, num=20, mean_std=None):\n",
    "    \"\"\"Confusion Matrix 출력을 위한 공간.\n",
    "\n",
    "    Args:\n",
    "        pic (List:[tuple(wrong_imgs, wrong_answers, wrong_labels)]): inference_model의 결과값.\n",
    "        num (int, optional): 출력하고 싶은 이미지 갯수. Defaults to 20.\n",
    "        mean_std (tuple, optional): ((Mean), (Std))). Defaults to None.\n",
    "    \"\"\"\n",
    "    # 출력할 때마다 다른 이미지 보여지도록 추가한 라인.\n",
    "    pic = random.choices(list(pic), k=num)\n",
    "    \n",
    "    for idx, (img, true, pred) in enumerate(pic):\n",
    "        # 위에서 random.choices로 갯수 제한해서 필요없긴 한데 갯수 제한없이 출력했을 때,\n",
    "        # 컴퓨터 터질 뻔해서 이중으로 대비했습니다.\n",
    "        if idx >= num:\n",
    "            break\n",
    "        \n",
    "        # plt.imshow는 (H, W, C)만 출력 가능해서 추가한 라인.\n",
    "        img = rearrange(img, 'c h w -> h w c')\n",
    "        \n",
    "        # UnNormalize를 위해 추가한 코드.\n",
    "        if mean_std:\n",
    "            trfm = UnNormalize(*mean_std)\n",
    "            img = trfm(img)\n",
    "        \n",
    "        # 이미지 출력 코드.\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'num: {idx+1} answer: {true}, pred: {pred}')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/bc-ai-recsys3-lv1-imgclassification\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset, random_split, WeightedRandomSampler\n",
    "# from torchvision import transforms\n",
    "# from torchvision.transforms import *\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    \".jpg\", \".JPG\", \".jpeg\", \".JPEG\", \".png\",\n",
    "    \".PNG\", \".ppm\", \".PPM\", \".bmp\", \".BMP\",\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, resize, mean, std, **args):\n",
    "        self.transform = Compose([\n",
    "            CenterCrop(320, 256, p=1.),\n",
    "            Resize(resize[0], resize[1], Image.BILINEAR, p=1.),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255., p=1.),\n",
    "            ToTensorV2(p=1.),\n",
    "        ], p=1.)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image=image)\n",
    "\n",
    "\n",
    "class BaseAugmentation:\n",
    "    def __init__(self, resize, mean, std, **args):\n",
    "        self.transform = Compose([\n",
    "            CenterCrop(320, 256, p=1.),\n",
    "            Resize(resize[0], resize[1], Image.BILINEAR, p=1.),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255., p=1.),\n",
    "            ToTensorV2(p=1.),\n",
    "        ], p=1.)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image=image)\n",
    "\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "        transform 에 없는 기능들은 이런식으로 __init__, __call__, __repr__ 부분을\n",
    "        직접 구현하여 사용할 수 있습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "class CustomAugmentation:\n",
    "    def __init__(self, resize, mean, std, **args):\n",
    "        self.resize = resize\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform =  Compose([\n",
    "            CenterCrop(320, 256, p=1.),\n",
    "            Resize(resize[0], resize[1], Image.BILINEAR, p=1.),\n",
    "            ShiftScaleRotate(shift_limit=0.05, rotate_limit=20, p=.7),\n",
    "            RandomBrightnessContrast(p=.7),\n",
    "            OneOf([\n",
    "                FancyPCA(p=1.),\n",
    "                GaussNoise(p=.5),\n",
    "            ], p=1.),\n",
    "            Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image=image)\n",
    "\n",
    "\n",
    "class RandAugmentation:\n",
    "    def __init__(self, resize, mean, std, **args):\n",
    "        self.resize = resize\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform =  Compose([\n",
    "            ShiftScaleRotate(shift_limit=0.05, rotate_limit=20, p=.7),\n",
    "            RandomBrightnessContrast(p=.7),\n",
    "            OneOf([\n",
    "                FancyPCA(p=1.),\n",
    "                GaussNoise(p=.5),\n",
    "            ], p=1.),\n",
    "            # Normalize(mean=mean, std=std),\n",
    "            # ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image=image)\n",
    "    \n",
    "\n",
    "class MaskLabels(int, Enum):\n",
    "    MASK = 0\n",
    "    INCORRECT = 1\n",
    "    NORMAL = 2\n",
    "\n",
    "\n",
    "class GenderLabels(int, Enum):\n",
    "    MALE = 0\n",
    "    FEMALE = 1\n",
    "\n",
    "    @classmethod\n",
    "    def from_str(cls, value: str) -> int:\n",
    "        value = value.lower()\n",
    "        if value == \"male\":\n",
    "            return cls.MALE\n",
    "        elif value == \"female\":\n",
    "            return cls.FEMALE\n",
    "        else:\n",
    "            raise ValueError(f\"Gender value should be either 'male' or 'female', {value}\")\n",
    "\n",
    "\n",
    "class AgeLabels(int, Enum):\n",
    "    YOUNG = 0\n",
    "    MIDDLE = 1\n",
    "    OLD = 2\n",
    "\n",
    "    @classmethod\n",
    "    def from_number(cls, value: str) -> int:\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Age value should be numeric, {value}\")\n",
    "\n",
    "        if value < 30:\n",
    "            return cls.YOUNG\n",
    "        elif value < 60:\n",
    "            return cls.MIDDLE\n",
    "        else:\n",
    "            return cls.OLD\n",
    "\n",
    "\n",
    "class MaskBaseDataset(Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1\": MaskLabels.MASK,\n",
    "        \"mask2\": MaskLabels.MASK,\n",
    "        \"mask3\": MaskLabels.MASK,\n",
    "        \"mask4\": MaskLabels.MASK,\n",
    "        \"mask5\": MaskLabels.MASK,\n",
    "        \"incorrect_mask\": MaskLabels.INCORRECT,\n",
    "        \"normal\": MaskLabels.NORMAL\n",
    "    }\n",
    "    \n",
    "    image_paths = []\n",
    "    save_paths = []\n",
    "    \n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, data_dir, mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246), val_ratio=0.2):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.val_ratio = val_ratio # validation ratio \n",
    "        \n",
    "        self.transform = None\n",
    "        self.setup()\n",
    "        self.calc_statistics()\n",
    "        \n",
    "    def setup(self):\n",
    "        profiles = os.listdir(self.data_dir)\n",
    "        for profile in profiles:\n",
    "            if profile.startswith(\".\"):  # \".\" 로 시작하는 파일은 무시합니다\n",
    "                continue\n",
    "\n",
    "            img_folder = os.path.join(self.data_dir, profile)\n",
    "            for file_name in os.listdir(img_folder):\n",
    "                _file_name, ext = os.path.splitext(file_name)\n",
    "                if _file_name not in self._file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "                    continue\n",
    "                if ext != '.jpg':\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(self.data_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                save_path = os.path.join(self.data_dir, profile, _file_name)\n",
    "                mask_label = self._file_names[_file_name]\n",
    "\n",
    "                id, gender, race, age = profile.split(\"_\")\n",
    "                gender_label = GenderLabels.from_str(gender)\n",
    "                age_label = AgeLabels.from_number(age)\n",
    "\n",
    "                self.image_paths.append(img_path)\n",
    "                self.save_paths.append(save_path)\n",
    "                self.mask_labels.append(mask_label)\n",
    "                self.gender_labels.append(gender_label)\n",
    "                self.age_labels.append(age_label)\n",
    "\n",
    "    def calc_statistics(self):\n",
    "        has_statistics = self.mean is not None and self.std is not None\n",
    "        if not has_statistics:\n",
    "            print(\"[Warning] Calculating statistics... It can take a long time depending on your CPU machine\")\n",
    "            sums = []\n",
    "            squared = []\n",
    "            for image_path in self.image_paths[:3000]:\n",
    "                image = np.array(Image.open(image_path)).astype(np.int32)\n",
    "                sums.append(image.mean(axis=(0, 1)))\n",
    "                squared.append((image ** 2).mean(axis=(0, 1)))\n",
    "\n",
    "            self.mean = np.mean(sums, axis=0) / 255\n",
    "            self.std = (np.mean(squared, axis=0) - self.mean ** 2) ** 0.5 / 255\n",
    "            print(self.mean, self.std)\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert self.transform is not None, \".set_tranform 메소드를 이용하여 transform 을 주입해주세요\"\n",
    "\n",
    "        mask_label = self.get_mask_label(index)\n",
    "        gender_label = self.get_gender_label(index)\n",
    "        age_label = self.get_age_label(index)\n",
    "        multi_class_label = self.encode_multi_class(mask_label, gender_label, age_label)\n",
    "\n",
    "        image = self.read_image(index)\n",
    "        image_np = np.array(image)\n",
    "        image_transform = self.transform(image_np)['image']\n",
    "        return image_transform, multi_class_label #, self.save_paths[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def get_mask_label(self, index) -> MaskLabels:\n",
    "        return self.mask_labels[index]\n",
    "\n",
    "    def get_gender_label(self, index) -> GenderLabels:\n",
    "        return self.gender_labels[index]\n",
    "\n",
    "    def get_age_label(self, index) -> AgeLabels:\n",
    "        return self.age_labels[index]\n",
    "\n",
    "    def read_image(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        return np.array(Image.open(image_path))\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_multi_class(mask_label, gender_label, age_label) -> int:\n",
    "        return mask_label * 6 + gender_label * 3 + age_label\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_multi_class(multi_class_label) -> Tuple[MaskLabels, GenderLabels, AgeLabels]:\n",
    "        mask_label = (multi_class_label // 6) % 3\n",
    "        gender_label = (multi_class_label // 3) % 2\n",
    "        age_label = multi_class_label % 3\n",
    "        return mask_label, gender_label, age_label\n",
    "\n",
    "    @staticmethod\n",
    "    def denormalize_image(image, mean, std):\n",
    "        img_cp = image.copy()\n",
    "        img_cp *= std\n",
    "        img_cp += mean\n",
    "        img_cp *= 255.0\n",
    "        img_cp = np.clip(img_cp, 0, 255).astype(np.uint8)\n",
    "        return img_cp\n",
    "\n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        \"\"\"\n",
    "        데이터셋을 train 과 val 로 나눕니다,\n",
    "        pytorch 내부의 torch.utils.data.random_split 함수를 사용하여\n",
    "        torch.utils.data.Subset 클래스 둘로 나눕니다.\n",
    "        구현이 어렵지 않으니 구글링 혹은 IDE (e.g. pycharm) 의 navigation 기능을 통해 코드를 한 번 읽어보는 것을 추천드립니다^^\n",
    "        \"\"\"\n",
    "        n_val = int(len(self) * self.val_ratio)\n",
    "        n_train = len(self) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        return train_set, val_set\n",
    "\n",
    "\n",
    "class MaskSplitByProfileDataset(MaskBaseDataset):\n",
    "    \"\"\"\n",
    "        train / val 나누는 기준을 이미지에 대해서 random 이 아닌\n",
    "        사람(profile)을 기준으로 나눕니다.\n",
    "        구현은 val_ratio 에 맞게 train / val 나누는 것을 이미지 전체가 아닌 사람(profile)에 대해서 진행하여 indexing 을 합니다\n",
    "        이후 `split_dataset` 에서 index 에 맞게 Subset 으로 dataset 을 분기합니다.\n",
    "    \"\"\"\n",
    "    # calc_statistics self.image_paths[:3000]:\n",
    "    #   [0.5573112  0.52429302 0.50174594] [0.61373778 0.58633636 0.56743769]\n",
    "    def __init__(self, data_dir, label='multi', mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246), val_ratio=0.2):\n",
    "        self.indices = defaultdict(list)\n",
    "        self.label = label\n",
    "        self.multi_labels = []\n",
    "        self.downsample = True\n",
    "        \n",
    "        super().__init__(data_dir, mean, std, val_ratio)\n",
    "        \n",
    "        if self.label == 'multi':\n",
    "            self.num_classes = 3 * 2 * 3\n",
    "            self.target_label = self.multi_labels\n",
    "        elif self.label == 'mask':\n",
    "            self.num_classes = 3\n",
    "            self.target_label = self.mask_labels\n",
    "        elif self.label == 'gender':\n",
    "            self.num_classes = 2\n",
    "            self.target_label = self.gender_labels\n",
    "        elif self.label == 'age':\n",
    "            self.num_classes = 3\n",
    "            self.target_label = self.age_labels\n",
    "        else:\n",
    "            raise ValueError(f\"label must be 'multi', 'mask', 'gender', or 'age', {self.label}\")\n",
    "        self.class_weights = self.compute_class_weight()\n",
    "\n",
    "    def read_image(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        return np.load(image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_np = self.read_image(index)\n",
    "        # image_np = np.array(image)\n",
    "        image_transform = self.transform(image_np)['image']\n",
    "        return image_transform, int(self.target_label[index])\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_profile(profiles, val_ratio):\n",
    "        length = len(profiles)\n",
    "        n_val = int(length * val_ratio)\n",
    "\n",
    "        val_indices = set(random.sample(range(length), k=n_val))\n",
    "        train_indices = set(range(length)) - val_indices\n",
    "        return {\n",
    "            \"train\": train_indices,\n",
    "            \"val\": val_indices\n",
    "        }\n",
    "\n",
    "    def setup(self):\n",
    "        profiles = os.listdir(self.data_dir)\n",
    "        profiles = [profile for profile in profiles if not profile.startswith(\".\")]\n",
    "        split_profiles = self._split_profile(profiles, self.val_ratio)\n",
    "\n",
    "        cnt = 0\n",
    "        for phase, indices in split_profiles.items():\n",
    "            for _idx in indices:\n",
    "                include_mask = True\n",
    "                profile = profiles[_idx]\n",
    "                img_folder = os.path.join(self.data_dir, profile)\n",
    "                lst_dirs = os.listdir(img_folder)\n",
    "                random.shuffle(lst_dirs)\n",
    "                for file_name in lst_dirs:\n",
    "                    _file_name, ext = os.path.splitext(file_name)\n",
    "                    if _file_name not in self._file_names:  # \".\" 로 시작하는 파일 및 invalid 한 파일들은 무시합니다\n",
    "                        continue\n",
    "                    if ext != '.npy':\n",
    "                        continue\n",
    "                    if self.downsample and file_name.startswith('mask'):\n",
    "                        if not include_mask:\n",
    "                            continue\n",
    "                        include_mask = False # include only 1 mask image per profile\n",
    "\n",
    "                    img_path = os.path.join(self.data_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                    mask_label = self._file_names[_file_name]\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = GenderLabels.from_str(gender)\n",
    "                    age_label = AgeLabels.from_number(age)\n",
    "\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(mask_label)\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "                    self.multi_labels.append(self.encode_multi_class(mask_label, gender_label, age_label))\n",
    "\n",
    "                    self.indices[phase].append(cnt)\n",
    "                    cnt += 1\n",
    "\n",
    "    def split_dataset(self) -> List[Subset]:\n",
    "        return [Subset(self, indices) for phase, indices in self.indices.items()]\n",
    "    \n",
    "    def get_train_labels(self, label):\n",
    "        \"\"\"\n",
    "        returns train data of the input label\n",
    "        \"\"\"\n",
    "        train_index = self.indices['train']\n",
    "        return [label[idx] for idx in train_index]\n",
    "    \n",
    "    def get_classweight_label(self, label) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        returns class weight of a label within train dataset\n",
    "        \"\"\"\n",
    "        train_labels = self.get_train_labels(label)\n",
    "        _, n_samples = np.unique(train_labels, return_counts=True)\n",
    "        weights = 1. / torch.tensor(n_samples, dtype=torch.float)\n",
    "        return weights\n",
    "    \n",
    "    def normalize_weight(self, n_samples):\n",
    "        norm_weights = [1 - (sample / sum(n_samples)) for sample in n_samples]\n",
    "        return torch.tensor(norm_weights, dtype=torch.float)\n",
    "\n",
    "    ##################### need refactoring ##################### \n",
    "    def get_weighted_sampler(self) -> WeightedRandomSampler:  \n",
    "        \"\"\"\n",
    "        returns WeightedRandomSampler based on the distribution of the train label\n",
    "        used to prevent overfitting due to unbalanced dataset\n",
    "        \"\"\"\n",
    "        # # v0: weights on target label\n",
    "        # train_index = self.indices['train'] # indices of train dataset\n",
    "        # train_labels = [self.target_label[idx] for idx in train_index] # target_label of train dataset\n",
    "        # class_counts = np.array([len(np.where(train_labels==t)[0]) for t in np.unique(train_labels)]) # get counts of each class \n",
    "        # weights = 1. / torch.tensor(class_counts, dtype=torch.float) # get weights (more class count == less weight(frequent) it will be sampled)\n",
    "        # samples_weights = weights[train_labels] # map weights for each train dataset, len(samples_weights) == len(train dataset)\n",
    "        # return WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "        \n",
    "        # # v1: normalized weights on target label (better than v0)\n",
    "        sample_weight = [self.class_weights[self.target_label[idx]] for idx in self.indices['train']]\n",
    "        return WeightedRandomSampler(weights=sample_weight, num_samples=len(sample_weight), replacement=True)\n",
    "        \n",
    "        # # v2: normalized weights on of specific ratio ``age=.9 : gender=.1``\n",
    "        # age_weight = self.get_classweight_label(self.age_labels)\n",
    "        # age_weight = [age_weight[self.age_labels[idx]] for idx in self.indices['train']]\n",
    "        # gender_weight = self.get_classweight_label(self.gender_labels)\n",
    "        # gender_weight = [gender_weight[self.gender_labels[idx]] for idx in self.indices['train']]\n",
    "        # weights = .9*age_weight + .1*gender_weight\n",
    "        # return WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "    def compute_class_weight(self) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        estimate class weights for unbalanced dataset\n",
    "        `` 1 - n_sample / sum(n_samples) ````\n",
    "        used for loss function: weighted_cross_entropy\n",
    "        \"\"\"\n",
    "        train_index = self.indices['train']\n",
    "        train_labels = [self.target_label[idx] for idx in train_index]\n",
    "        _, n_samples = np.unique(train_labels, return_counts=True)\n",
    "        norm_weights = [1 - (sample / sum(n_samples)) for sample in n_samples]\n",
    "        return torch.tensor(norm_weights, dtype=torch.float).to(device='cuda')\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, resize, mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = Compose([\n",
    "            CenterCrop(320, 256, p=1.),\n",
    "            Resize(resize[0], resize[1], Image.BILINEAR),\n",
    "            Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image_np = np.array(image)\n",
    "            trans_image = self.transform(image=image_np)['image']\n",
    "        return trans_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset import MaskSplitByProfileDataset, BaseAugmentation\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_dataloader(label: str='multi'):\n",
    "    seed_everything(42)\n",
    "\n",
    "    # -- settings\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # -- dataset\n",
    "    data_dir = '/opt/ml/input/data/train/images'\n",
    "    dataset = MaskBaseDataset(\n",
    "        data_dir=data_dir,\n",
    "    )\n",
    "    \n",
    "    # -- augmentation\n",
    "    resize = [224, 224]\n",
    "    transform = BaseAugmentation(\n",
    "        resize=resize,\n",
    "        mean=dataset.mean,\n",
    "        std=dataset.std,\n",
    "    )\n",
    "    dataset.set_transform(transform)\n",
    "\n",
    "    # -- data_loader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        # batch_size=args.batch_size,\n",
    "        num_workers=multiprocessing.cpu_count()//2,\n",
    "        # shuffle=True,\n",
    "        pin_memory=use_cuda,\n",
    "        drop_last=True,\n",
    "        # sampler=sampler,\n",
    "    )\n",
    "    \n",
    "    # resize = [224, 224]\n",
    "    # batch_size = 1000\n",
    "\n",
    "    # data_dir = '/opt/ml/input/data/train/images'\n",
    "    # dataset = MaskSplitByProfileDataset(data_dir, label, resize)\n",
    "    # transform = BaseAugmentation(resize, dataset.mean, dataset.std)\n",
    "    # dataset.set_transform(transform)\n",
    "    # dataloader = torch.utils.data.DataLoader(\n",
    "    #         dataset,\n",
    "    #         # batch_size=batch_size,\n",
    "    #         # num_workers=8,\n",
    "    #         shuffle=False,\n",
    "    #         pin_memory=torch.cuda.is_available(),\n",
    "    #         drop_last=False,\n",
    "    #     )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: tensor([[[[ 1.4439,  1.4439,  1.4439,  ...,  0.8482,  0.8647,  0.8647],\n",
      "          [ 1.4439,  1.4439,  1.4439,  ...,  0.7158,  0.7158,  0.7158],\n",
      "          [ 1.4439,  1.4439,  1.4439,  ...,  0.5834,  0.5669,  0.5669],\n",
      "          ...,\n",
      "          [-0.9389, -0.3432, -0.3928,  ..., -1.3029, -1.1374, -1.0878],\n",
      "          [-1.2532, -0.6410, -0.7568,  ..., -1.1209, -0.8065, -0.7403],\n",
      "          [-1.0712, -0.4756, -0.6410,  ..., -1.2367, -1.0216, -1.0381]],\n",
      "\n",
      "         [[ 1.7064,  1.7064,  1.7064,  ..., -0.5798, -0.5639, -0.5639],\n",
      "          [ 1.7064,  1.7064,  1.7064,  ..., -0.4211, -0.4052, -0.4052],\n",
      "          [ 1.7064,  1.7064,  1.7064,  ..., -0.0241, -0.0083,  0.0076],\n",
      "          ...,\n",
      "          [-0.7545, -0.1829, -0.2464,  ..., -1.2784, -1.1514, -1.1514],\n",
      "          [-1.1038, -0.5163, -0.6433,  ..., -1.1038, -0.8338, -0.8180],\n",
      "          [-0.9926, -0.4052, -0.5639,  ..., -1.2149, -1.0402, -1.1038]],\n",
      "\n",
      "         [[ 1.7672,  1.7672,  1.7672,  ..., -0.0183, -0.0023, -0.0023],\n",
      "          [ 1.7672,  1.7672,  1.7672,  ...,  0.0455,  0.0615,  0.0615],\n",
      "          [ 1.7672,  1.7672,  1.7672,  ...,  0.2687,  0.2846,  0.2846],\n",
      "          ...,\n",
      "          [-0.8153, -0.2255, -0.2893,  ..., -1.2139, -1.1023, -1.0863],\n",
      "          [-1.1501, -0.5443, -0.6400,  ..., -1.0544, -0.7834, -0.7516],\n",
      "          [-1.0066, -0.4008, -0.5603,  ..., -1.1660, -0.9907, -1.0385]]]]), tensor([13])\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_dataloader()\n",
    "for idx, data in enumerate(dataloader):\n",
    "    print(f'{idx}: {data[0]}, {data[1]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import parse_model_param\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, script: str):\n",
    "        self.script = script\n",
    "        self.model_param = self.parse_script()\n",
    "        self.num_classes_label = {\n",
    "            'multi': 18,\n",
    "            'age': 3,\n",
    "            'gender': 2,\n",
    "            'mask': 3\n",
    "        }\n",
    "        self.ans, self.preds, self.wrong = self.inference(self.model_param)\n",
    "\n",
    "    def parse_script(self, script: str=None):\n",
    "        \"\"\"\n",
    "        takes as input a script to generate a model\n",
    "        returns a prediction made using the model\n",
    "        \"\"\"\n",
    "        # i.e. python train.py --epochs 120 --dataset MaskSplitByProfileDataset --augmentation CustomAugmentation --model PretrainedModels --model_param resnet false --optimizer SGD --name ResNet_Ep120_SplitProf_Downsample_CustAugv1_WeightedCEnSamplev2_SGD_MASK --label mask\n",
    "        if script is None:\n",
    "            script = self.script\n",
    "        \n",
    "        model_param = {}\n",
    "        lst_param = script.split(sep=' ')\n",
    "        # default params\n",
    "        model_param['label'] = 'multi'\n",
    "        for idx, param in enumerate(lst_param):\n",
    "            if param == '--model':\n",
    "                model_param['model'] = lst_param[idx+1]\n",
    "            if param == '--label':\n",
    "                model_param['label'] = lst_param[idx+1]\n",
    "            if param == '--name':\n",
    "                model_param['name'] = lst_param[idx+1]\n",
    "            if param == '--model_param':\n",
    "                model_param['model_param'] = lst_param[idx+1]\n",
    "        model_param['model_dir'] = os.path.join('./model', model_param['label'], model_param['name'])\n",
    "        # print(model_param)\n",
    "        return model_param\n",
    "\n",
    "    def inference(self, model_param: dict=None):\n",
    "        if model_param is None:\n",
    "            model_param = self.model_param\n",
    "\n",
    "        model_dir = self.model_param['model_dir']\n",
    "        label = self.model_param['label']\n",
    "        model = self.model_param['model']\n",
    "        \n",
    "        self.model_param['num_classes'] = self.num_classes_label[label]\n",
    "        params = parse_model_param(self.model_param['model_param'], pretrained=True)\n",
    "\n",
    "        dataloader = get_dataloader(label)\n",
    "        preds = inference_model(dataloader, model_dir, model, self.num_classes_label[label], break_idx=3000, **params)\n",
    "        return preds\n",
    "\n",
    "    def display_cfmatrix(self):\n",
    "        display_cfmatrix(\n",
    "            self.model_param['label'], \n",
    "            self.model_param['num_classes'], \n",
    "            self.ans,\n",
    "            self.pred\n",
    "        )\n",
    "    \n",
    "    def display_incorrect(self, n_display: int=10):\n",
    "        show(self.wrong, n_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/234455 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inference results for ./model/age/ResNet_Ep120_SplitProf_Downsample_CustAugv1_WeightedCEnSamplev2_SGD_AGE..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1887/234455 [00:42<1:28:15, 43.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-5015d7b5e37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model_param = parse_script(script)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ans, preds, wrong = get_preds(model_param)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mage_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_age\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-9631aa35f876>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, script)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;34m'mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         }\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-9631aa35f876>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, model_param)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-100100e24fcf>\u001b[0m in \u001b[0;36minference_model\u001b[0;34m(loader, model_dir, model, num_classes, break_idx, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# <계산 과정>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# </계산 과정>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bc-ai-recsys3-lv1-imgclassification/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "script_age = 'python train.py --epochs 120 --dataset MaskSplitByProfileDataset --augmentation CustomAugmentation --model PretrainedModels --model_param resnet false --optimizer SGD --name ResNet_Ep120_SplitProf_Downsample_CustAugv1_WeightedCEnSamplev2_SGD_AGE --label age'\n",
    "# model_param = parse_script(script)\n",
    "# ans, preds, wrong = get_preds(model_param)\n",
    "age_eval = ModelEvaluator(script_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cfmatrix(model_param['label'], model_param['num_classes'], ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(x, num=20, mean_std=(0.5, 1.2))\n",
    "# show(preds, num=10)\n",
    "# show(preds, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/162315 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inference results for ./model/gender/ResNet_Ep120_SplitProf_Downsample_CustAugv1_WeightedCEnSamplev2_SGD_GENDER..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 300/162315 [00:04<43:59, 61.38it/s] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x = inference_model(\n",
    "#     DL, \n",
    "#     '/opt/ml/workspace/saved/UniGakGakDataset_BaseAugmentation_ResNetModel_SGD/mask', \n",
    "#     3, \"ResNetModel\", break_idx=100\n",
    "#     )\n",
    "script = 'python train.py --epochs 120 --dataset MaskSplitByProfileDataset --augmentation CustomAugmentation --model PretrainedModels --model_param resnet false --optimizer SGD --name ResNet_Ep120_SplitProf_Downsample_CustAugv1_WeightedCEnSamplev2_SGD_GENDER --label gender'\n",
    "model_param = parse_script(script)\n",
    "preds = get_preds(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180350 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating inference results for ./model/mask/ResNet_Ep60_SplitProf_Downsample_CustAugv1_WeightedCEnSample_SGD_MASK..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 300/180350 [00:09<1:34:02, 31.91it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "script = 'python train.py --epochs 120 --dataset MaskSplitByProfileDataset --augmentation CustomAugmentation --model PretrainedModels --model_param resnet false --optimizer SGD --name ResNet_Ep60_SplitProf_Downsample_CustAugv1_WeightedCEnSample_SGD_MASK --label mask'\n",
    "model_param = parse_script(script)\n",
    "preds = get_preds(model_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Fit the model\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f26f67ee040>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
